import streamlit as st
import torch
import time
import os
import glob
import random

# --- ä»é¡¹ç›®ä¸­å¯¼å…¥å¿…è¦çš„æ¨¡å— ---
from config import config, model_config
from Loader.SeriesLoader import TimeSeriesDataset
from utils.predictior import ModelInference

# --- åº”ç”¨é…ç½® ---
st.set_page_config(page_title="å¤šç®¡é“åˆ‡æ¢é¢„æµ‹æ¨¡æ‹Ÿ (v4)", layout="centered")

MODEL_PATH = "FFTbest_classification_model.pth"
CLASS_NAMES = ["CC", "GL", "LC", "OL", "NL"]
NUM_PIPELINES = 100  # è¦éšæœºé€‰æ‹©çš„ç®¡é“ï¼ˆæ–‡ä»¶ï¼‰æ•°é‡
SIMULATION_INTERVAL_SECONDS = 0 # æ¨¡æ‹Ÿé€Ÿåº¦

# --- å…³é”®å‡½æ•° (ç¼“å­˜ä»¥æé«˜æ€§èƒ½) ---

@st.cache_resource
def load_resources():
    """
    åŠ è½½æ¨¡å‹å’Œæ•°æ®ã€‚
    1. éšæœºé€‰æ‹© N ä¸ªæ•°æ®æ–‡ä»¶ã€‚
    2. ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ TimeSeriesDataset å¯¹è±¡ã€‚
    3. å­˜å‚¨æ¯ä¸ªç®¡é“çš„ Dataset å¯¹è±¡ã€æ ·æœ¬æ•°å’ŒçœŸå®æ ‡ç­¾ã€‚
    """
    # 1. åˆå§‹åŒ–æ¨¡å‹æ¨ç†æ¥å£
    try:
        predictor = ModelInference(
            model_path=MODEL_PATH,
            config=config,
            model_config=model_config,
            class_names=CLASS_NAMES
        )
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return None, None

    # 2. æŸ¥æ‰¾å¹¶éšæœºé€‰æ‹©æ–‡ä»¶
    try:
        all_file_paths = sorted(glob.glob(os.path.join(config["data_dir"], config["file_pattern"])))
        if not all_file_paths:
            st.error(f"åœ¨ '{config['data_dir']}' ä¸­æœªæ‰¾åˆ°åŒ¹é… '{config['file_pattern']}' çš„æ–‡ä»¶ã€‚")
            return predictor, None

        if len(all_file_paths) < NUM_PIPELINES:
            st.warning(f"æ–‡ä»¶ä¸è¶³ {NUM_PIPELINES} ä¸ªï¼Œå°†ä½¿ç”¨æ‰€æœ‰ {len(all_file_paths)} ä¸ªå¯ç”¨æ–‡ä»¶ã€‚")
            paths_to_simulate = all_file_paths
        else:
            paths_to_simulate = random.sample(all_file_paths, NUM_PIPELINES)
        
        pipeline_info = []
        st.info("æ­£åœ¨ä¸ºæ¯ä¸ªé€‰ä¸­çš„ç®¡é“åˆ›å»ºç‹¬ç«‹çš„ Dataset...")

        for path in paths_to_simulate:
            # --- æ ¸å¿ƒæŠ€å·§ï¼šä½¿ç”¨æ–‡ä»¶åä½œä¸º file_pattern æ¥åŠ è½½å•ä¸ªæ–‡ä»¶ ---
            data_dir = os.path.dirname(path)
            file_pattern = os.path.basename(path)
            
            dataset_for_pipeline = TimeSeriesDataset(
                data_dir=data_dir,
                file_pattern=file_pattern, # <--- æ­£ç¡®ç”¨æ³•
                window_size=config["window_size"],
                stride=config["stride"],
                target_col=config["target_col"],
                Normalization=config.get("Normalization", True),
                delete_col=config["delete_col"],
                constant_col=config["constant_col"]
            )
            
            # å¦‚æœè¯¥æ–‡ä»¶èƒ½ç”Ÿæˆæœ‰æ•ˆæ ·æœ¬ï¼Œåˆ™å­˜å‚¨å…¶ä¿¡æ¯
            if len(dataset_for_pipeline) > 0:
                _, label_tensor = dataset_for_pipeline[0] # ä»ç¬¬ä¸€ä¸ªæ ·æœ¬è·å–æ ‡ç­¾
                pipeline_info.append({
                    'path': file_pattern,
                    'dataset': dataset_for_pipeline, # <--- ç›´æ¥å­˜å‚¨Datasetå¯¹è±¡
                    'num_samples': len(dataset_for_pipeline),
                    'true_label_idx': label_tensor.item()
                })
        
        if not pipeline_info:
            st.error("æ‰€æœ‰é€‰ä¸­çš„æ–‡ä»¶éƒ½æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„æ•°æ®æ ·æœ¬ã€‚")
            return predictor, None

        st.success("æ¨¡å‹å’Œæ•°æ®èµ„æºåŠ è½½å®Œæˆï¼")
        return predictor, pipeline_info

    except Exception as e:
        st.error(f"å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return predictor, None

# --- Streamlit åº”ç”¨ä¸»é€»è¾‘ ---

# 1. åŠ è½½æ‰€æœ‰èµ„æº
predictor, pipeline_info = load_resources()
if not predictor or not pipeline_info:
    st.stop()

# 2. åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'selected_pipeline_index' not in st.session_state:
    st.session_state.selected_pipeline_index = 0
if 'correct_predictions' not in st.session_state:
    st.session_state.correct_predictions = 0
if 'total_predictions' not in st.session_state:
    st.session_state.total_predictions = 0

# 3. ä¾§è¾¹æ ï¼šç”¨äºé€‰æ‹©ç®¡é“
with st.sidebar:
    st.header("ç®¡é“æ§åˆ¶é¢æ¿")
    pipeline_names = [f"ç®¡é“ {i+1} ({info['path']})" for i, info in enumerate(pipeline_info)]

    for i, name in enumerate(pipeline_names):
        if st.button(name, key=f"pipe_{i}"):
            if st.session_state.selected_pipeline_index != i:
                st.session_state.selected_pipeline_index = i
                st.session_state.correct_predictions = 0
                st.session_state.total_predictions = 0
                st.rerun()

# --- 4. ä¸»ç•Œé¢å¸ƒå±€å’Œæ¨¡æ‹Ÿ ---

# è·å–å½“å‰é€‰æ‹©çš„ç®¡é“ä¿¡æ¯
selected_idx = st.session_state.selected_pipeline_index
current_pipeline = pipeline_info[selected_idx]

# ä»ç®¡é“ä¿¡æ¯ä¸­ç›´æ¥è·å– Dataset å¯¹è±¡å’Œç›¸å…³å‚æ•°
dataset_to_simulate = current_pipeline['dataset']
true_label = CLASS_NAMES[current_pipeline['true_label_idx']]
total_samples_in_pipeline = current_pipeline['num_samples']

st.title(f"æ­£åœ¨æ¨¡æ‹Ÿ: {current_pipeline['path']}")
st.markdown(f"**çœŸå®çŠ¶æ€:** `{true_label}` | **æ­¤ç®¡é“æ ·æœ¬æ€»æ•°:** `{total_samples_in_pipeline}`")
st.markdown("---")

# 5. åˆ›å»ºç”¨äºåŠ¨æ€æ›´æ–°çš„å ä½ç¬¦
status_placeholder = st.empty()
accuracy_placeholder = st.empty()
result_placeholder = st.empty()

# 6. å®æ—¶æ¨¡æ‹Ÿå¾ªç¯ï¼Œç›´æ¥éå†å½“å‰ç®¡é“çš„ Dataset
for i, (inputs_tensor, _) in enumerate(dataset_to_simulate):
    # --- æ¨¡å‹é¢„æµ‹ ---
    inputs_tensor = inputs_tensor.unsqueeze(0).to(predictor.device)
    
    with torch.no_grad():
        outputs = predictor.model(inputs_tensor)
    
    _, predicted_idx = torch.max(outputs.data, 1)
    predicted_label = predictor.class_names[predicted_idx.item()]

    # æ›´æ–°å‡†ç¡®ç‡è®¡æ•°å™¨
    st.session_state.total_predictions += 1
    if predicted_label == true_label:
        st.session_state.correct_predictions += 1
    
    # --- æ›´æ–°ç•Œé¢å…ƒç´  ---
    status_text = "ğŸŸ¢ **æ¨¡æ‹Ÿè¿›è¡Œä¸­...**" if i < total_samples_in_pipeline - 1 else "ğŸ **æ¨¡æ‹Ÿå®Œæˆ!**"
    status_placeholder.header(f"{status_text}")
    status_placeholder.progress((i + 1) / total_samples_in_pipeline, text=f"å·²å¤„ç†æ ·æœ¬: {i + 1} / {total_samples_in_pipeline}")

    correct = st.session_state.correct_predictions
    total = st.session_state.total_predictions
    accuracy = (correct / total) * 100 if total > 0 else 0
    
    with accuracy_placeholder.container():
        st.metric(label="**å½“å‰ç®¡é“å‡†ç¡®ç‡**", value=f"{accuracy:.2f} %", delta=f"{correct} / {total} æ­£ç¡®")

    with result_placeholder.container():
        st.subheader("æœ€æ–°é¢„æµ‹ç»“æœ")
        col_pred, col_true = st.columns(2)
        with col_pred:
            st.write("**æ¨¡å‹é¢„æµ‹:**")
            if predicted_label == true_label:
                st.success(f"âœ”ï¸ {predicted_label}")
            else:
                st.error(f"âŒ {predicted_label}")
        with col_true:
            st.write("**çœŸå®çŠ¶æ€:**")
            st.info(f"â„¹ï¸ {true_label}")
    
    # æ§åˆ¶æ¨¡æ‹Ÿé€Ÿåº¦
    time.sleep(SIMULATION_INTERVAL_SECONDS)

# å¾ªç¯ç»“æŸåï¼Œç¡®ä¿æœ€ç»ˆçŠ¶æ€å¯è§
status_placeholder.header("ğŸ **æ¨¡æ‹Ÿå®Œæˆ!**")